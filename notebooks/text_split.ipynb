{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6a6f9fc-e749-4773-92b1-86b1eecbf629",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95cdd8b8-7009-4d5c-93d2-0d234914af76",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_processed_pdfs_sql=\"\"\"\n",
    "SELECT *  \n",
    "FROM papers\n",
    "INNER JOIN papers_raw_md USING (id) \n",
    "WHERE papers.extract_done=TRUE AND papers.transform_done=FALSE;\n",
    "\"\"\"\n",
    "\n",
    "with duckdb.connect('../data.duckdb')  as  con:\n",
    "    nonprocessed_pdfs=con.execute(non_processed_pdfs_sql).df()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "228f37db-32e9-49d2-83dc-97ce75b3f7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sql=\"\"\"\n",
    "SELECT *  \n",
    "FROM papers_chunks\n",
    "\"\"\"\n",
    "\n",
    "with duckdb.connect('../data.duckdb')  as  con:\n",
    "    data=con.execute(data_sql).df()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "524e2ed9-8dd8-43aa-b086-36060cab3c05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "#@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@#\n",
      "\n",
      "M proposals in Ri. This generates a set of negative-chips for\n",
      "each scale per image, Ci\n",
      "neg. During training, we randomly sample\n",
      "a ﬁxed number of negative chips per epoch (per image) from\n",
      "this pool of negative-chips at all scales, i.e. ⋃n\n",
      "i=1 Ci\n",
      "neg. Figure\n",
      "7 shows examples of the generated negative chips by SNIPER.\n",
      "The ﬁrst row shows the image and the ground-truth boxes, the\n",
      "bottom shows the proposals that are not covered by Ci\n",
      "pos and\n",
      "the corresponding negative-chips (the orange boxes). For clarity,\n",
      "we represent each proposal by a red circle in its center. We can\n",
      "see that SNIPER only processes regions which can likely contain\n",
      "false positives, while safely ignoring large portions of the image,\n",
      "which leads to faster processing time. Intuitively, this process is\n",
      "akin to hard-negative mining for those chips that contain difﬁcult\n",
      "background regions.\n",
      "3.3.4 Label Assignment\n",
      "Once the negative and positive chips are selected, our network \n",
      "#@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@#\n",
      "\n",
      "which leads to faster processing time. Intuitively, this process is\n",
      "akin to hard-negative mining for those chips that contain difﬁcult\n",
      "background regions.\n",
      "3.3.4 Label Assignment\n",
      "Once the negative and positive chips are selected, our network\n",
      "is trained end-to-end on these chips like Faster-RCNN, i.e. it\n",
      "learns to generate proposals as well as classify them with a single\n",
      "network. While training, proposals generated by RPN are assigned\n",
      "labels and bounding box targets (for regression) based on all the\n",
      "ground-truth boxes that are present inside the chip. We do not\n",
      "ﬁlter ground-truth boxes based on Ri. Instead, the proposals that\n",
      "don’t fall in Ri are ignored during training, or their gradients are\n",
      "not back-propagated. Therefore, a large ground-truth box that is\n",
      "cropped, can generate a valid proposal that is small. Like Fast-\n",
      "RCNN, we assign a positive label and bounding-box targets to all\n",
      "the proposals that have an overlap greater than 0.5 with a ground- \n",
      "#@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@#\n",
      "\n",
      "not back-propagated. Therefore, a large ground-truth box that is\n",
      "cropped, can generate a valid proposal that is small. Like Fast-\n",
      "RCNN, we assign a positive label and bounding-box targets to all\n",
      "the proposals that have an overlap greater than 0.5 with a ground-\n",
      "truth box. Our network is trained end-to-end and we generate 300\n",
      "proposals per chip. We do not constraint any fraction of these\n",
      "proposals for re-sampling as positives [50], as it’s done in Fast-\n",
      "RCNN. We did not use OHEM [51] for classiﬁcation, instead,\n",
      "we use a simple softmax cross-entropy loss for classiﬁcation. For\n",
      "assigning the RPN labels, we use valid ground-truth boxes to\n",
      "assign labels and invalid ground-truth boxes to invalidate anchors,\n",
      "as it’s done in SNIP.\n",
      "3.3.5 Beneﬁts\n",
      "For training, we randomly sample chips from the whole dataset\n",
      "for generating a batch. On average, we generate ∼ 5 chips of\n",
      "size 512x512 per image on the COCO dataset (including negative\n",
      "chips) when training on three scales ( 512/ms 2, 1.667, 3). This \n",
      "#@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@#\n",
      "\n",
      "as it’s done in SNIP.\n",
      "3.3.5 Beneﬁts\n",
      "For training, we randomly sample chips from the whole dataset\n",
      "for generating a batch. On average, we generate ∼ 5 chips of\n",
      "size 512x512 per image on the COCO dataset (including negative\n",
      "chips) when training on three scales ( 512/ms 2, 1.667, 3). This\n",
      "is only 30% more than the number of pixels processed per image\n",
      "when single scale training is performed with an image resolution\n",
      "of 800x1333. Since all our images are of the same size, data is\n",
      "much better packed leading to better GPU utilization which easily\n",
      "overcomes the extra30% overhead. But more importantly,we reap\n",
      "the beneﬁts of multi-scale training on 3 scales, large batch size\n",
      "and training with batch-normalization without any slowdown in\n",
      "performance on a single 8 GPU node! .\n",
      "It is commonly believed that high-resolution images ( e.g.\n",
      "800x1333) are necessary for instance-level recognition tasks.\n",
      "Therefore, for instance-level recognition tasks, it was not possible \n",
      "#@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@#\n",
      "\n",
      "and training with batch-normalization without any slowdown in\n",
      "performance on a single 8 GPU node! .\n",
      "It is commonly believed that high-resolution images ( e.g.\n",
      "800x1333) are necessary for instance-level recognition tasks.\n",
      "Therefore, for instance-level recognition tasks, it was not possible\n",
      "to train with batch-normalization statistics computed on a single\n",
      "GPU. Methods like synchronized batch-normalization [31], [52]\n",
      "or training on 128 GPUs [53] have been proposed to alleviate this\n",
      "problem. Synchronized batch-normalization slows down training\n",
      "signiﬁcantly and training on 128 GPUs is also impractical for most\n",
      "people. Therefore, group normalization [54] has been recently\n",
      "proposed so that instance-level recognition tasks can beneﬁt from\n",
      "another form of normalization in a low batch setting during train-\n",
      "ing. With SNIPER, we show that the image resolution bottleneck\n",
      "can be alleviated for instance-level recognition tasks. As long as \n",
      "#@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@#\n",
      "\n",
      "proposed so that instance-level recognition tasks can beneﬁt from\n",
      "another form of normalization in a low batch setting during train-\n",
      "ing. With SNIPER, we show that the image resolution bottleneck\n",
      "can be alleviated for instance-level recognition tasks. As long as\n",
      "we can cover negatives and use appropriate scale normalization\n",
      "methods, we can train with a large batch size of resampled low-\n",
      "resolution chips, even on challenging datasets like COCO. Our\n",
      "results suggest that context beyond a certain ﬁeld of view may not\n",
      "be beneﬁcial during training. It is also possible that the effective\n",
      "receptive ﬁeld of deep neural networks is not large enough to\n",
      "leverage far away pixels in the image, as suggested in [55].\n",
      "2. max(widthim,heightim)\n",
      "\f",
      "9\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "Small Med Large BG\n",
      "Area Percentage/Image0.3% 3%\n",
      "40%\n",
      "57%\n",
      "Small: (0, 322]\n",
      "Med: (322, 962] \n",
      "Large: (962, ∞)\n",
      "Fig. 8: Area of objects of different sizes and the background in the\n",
      "COCO validation set. Objects are divided based on their area (in \n",
      "#@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@#\n",
      "\n",
      "2. max(widthim,heightim)\n",
      "\f",
      "9\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "Small Med Large BG\n",
      "Area Percentage/Image0.3% 3%\n",
      "40%\n",
      "57%\n",
      "Small: (0, 322]\n",
      "Med: (322, 962] \n",
      "Large: (962, ∞)\n",
      "Fig. 8: Area of objects of different sizes and the background in the\n",
      "COCO validation set. Objects are divided based on their area (in\n",
      "pixels) into small, medium, and large.\n",
      "3.4 AutoFocus\n",
      "While multi-scale processing brings signiﬁcant improvements in\n",
      "accuracy, it comes at a computational cost, especially during\n",
      "inference. This is because the CNN is applied on all scales\n",
      "without factoring the spatial layout of the scene. To provide\n",
      "some perspective, we show the percentage of pixels occupied per\n",
      "image for different size objects in the COCO dataset in Fig 8.\n",
      "Even though 40% of the object instances are small, they only\n",
      "occupy 0.3% of the area. If the image pyramid includes a scale\n",
      "of 3, then just to detect such a small fraction of the dataset, we\n",
      "end up performing 9 times more computation at ﬁner-scales. If \n",
      "#@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@#\n",
      "\n",
      "Even though 40% of the object instances are small, they only\n",
      "occupy 0.3% of the area. If the image pyramid includes a scale\n",
      "of 3, then just to detect such a small fraction of the dataset, we\n",
      "end up performing 9 times more computation at ﬁner-scales. If\n",
      "we add some padding around small objects to provide spatial\n",
      "context and only upsample these regions, their area would still\n",
      "be small compared to the resolution of the original image. So,\n",
      "when performing multi-scale inference, can we predict regions\n",
      "containing small objects from coarser scales?\n",
      "So far, we have exploited the semantic layout of training\n",
      "images and ground-truth bounding-boxes to efﬁciently sample\n",
      "medium-sized chips, 512 ×512 pixels, from a multi-scale image\n",
      "pyramid to constrain the scale-range of training object instances\n",
      "and reduce computation. Unfortunately, this technique is not\n",
      "applicable to the inference stage due to the lack of ground-truth\n",
      "information, which leads to large inference-time computation. \n",
      "#@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@#\n",
      "\n",
      "pyramid to constrain the scale-range of training object instances\n",
      "and reduce computation. Unfortunately, this technique is not\n",
      "applicable to the inference stage due to the lack of ground-truth\n",
      "information, which leads to large inference-time computation.\n",
      "Fortunately, we are not the ﬁrst one to come across this problem\n",
      "and previous work has dealt with similar problems. Speciﬁcally,\n",
      "hand-crafted gradient-based features like SIFT [12] or SURF [56],\n",
      "combine two major components - the detector and the descriptor.\n",
      "The detector typically involves lightweight operators like Differ-\n",
      "ence of Gaussians (DoG) [57], Harris Afﬁne [58], Laplacian of\n",
      "Gaussians (LoG) [59] etc. and is applied to the complete image for\n",
      "ﬁnding interesting regions. The computationally heavy descriptor\n",
      "is only applied to interesting regions. Such cascaded processing of\n",
      "the image makes the entire pipeline computationally efﬁcient.\n",
      "We seek motivation from the aforementioned cascaded sys- \n",
      "#@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@#\n",
      "\n",
      "ﬁnding interesting regions. The computationally heavy descriptor\n",
      "is only applied to interesting regions. Such cascaded processing of\n",
      "the image makes the entire pipeline computationally efﬁcient.\n",
      "We seek motivation from the aforementioned cascaded sys-\n",
      "tems, and propose a novel framework that ﬁrst processes the\n",
      "coarsest scale and predicts the interesting regions in the image\n",
      "at the next scale. It continues processing the ﬁner-level scales, or\n",
      "higher resolutions, in a sequential manner and keeps predicting\n",
      "interesting regions at the next scale until the entire pyramid is\n",
      "not processed. It re-scales and crops only the detected interest-\n",
      "ing regions for applying compute-heavy detectors. AutoFocus is\n",
      "comprised of three main components: the ﬁrst learns to predict Fo-\n",
      "cusPixels, the second generates FocusChips for efﬁcient inference\n",
      "and the third merges detections from multiple scales, which we\n",
      "refer to as focus stacking for object detection. The details of each \n",
      "#@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@#\n",
      "\n",
      "comprised of three main components: the ﬁrst learns to predict Fo-\n",
      "cusPixels, the second generates FocusChips for efﬁcient inference\n",
      "and the third merges detections from multiple scales, which we\n",
      "refer to as focus stacking for object detection. The details of each\n",
      "of the components are described in the subsequent sections.\n",
      "3.4.1 FocusPixels\n",
      "FocusPixels are deﬁned at the granularity of the convolutional\n",
      "feature map (like conv5). A pixel in a feature map is labeled\n",
      "as a FocusPixel if it has any overlap with a small object. An\n",
      "object is considered small if it falls within a pre-deﬁned area range\n",
      "(between 5 ×5 and 64 ×64 pixels in our implementation) in a\n",
      "re-sized chip (Sec. 3.4.2). During training, FocusPixels are marked\n",
      "as positives. Pixels that overlap with objects even smaller than the\n",
      "small objects, ≤5×5 pixels are marked invalid. It’s because such\n",
      "objects become even smaller after down-sampling and the network\n",
      "doesn’t have sufﬁcient information to predict their location at the \n",
      "#@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@#\n",
      "\n",
      "as positives. Pixels that overlap with objects even smaller than the\n",
      "small objects, ≤5×5 pixels are marked invalid. It’s because such\n",
      "objects become even smaller after down-sampling and the network\n",
      "doesn’t have sufﬁcient information to predict their location at the\n",
      "next scale. We also mark the pixels that overlap with objects whose\n",
      "sizes range between 64 ×64 and 90 ×90 as invalid. It’s due to\n",
      "the fact that the transition from small to large objects doesn’t have\n",
      "a sharp boundary in terms of size. The rest of the feature-map\n",
      "pixels are marked as negative. AutoFocus is trained to generate\n",
      "high-value activations in the regions that contain FocusPixels.\n",
      "Formally, for an image of sizeX×Y , and a fully convolutional\n",
      "neural network with stride s, the resulting labels L will be of size\n",
      "X′×Y ′, where X′ = ⌈X\n",
      "s ⌉and Y ′ = ⌈Y\n",
      "s ⌉. Since the stride is\n",
      "s, each label l ∈L corresponds to s ×s pixels in the image. The\n",
      "label l is deﬁned as follows,\n",
      "l =\n",
      "\n",
      "\n",
      "\n",
      "1, IoU (GT, l) > 0, a <\n",
      "√\n",
      "GTArea < b \n",
      "#@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@#\n",
      "\n",
      "neural network with stride s, the resulting labels L will be of size\n",
      "X′×Y ′, where X′ = ⌈X\n",
      "s ⌉and Y ′ = ⌈Y\n",
      "s ⌉. Since the stride is\n",
      "s, each label l ∈L corresponds to s ×s pixels in the image. The\n",
      "label l is deﬁned as follows,\n",
      "l =\n",
      "\n",
      "\n",
      "\n",
      "1, IoU (GT, l) > 0, a <\n",
      "√\n",
      "GTArea < b\n",
      "−1, IoU (GT, l) > 0,\n",
      "√\n",
      "GTArea < a\n",
      "−1, IoU (GT, l) > 0, b <\n",
      "√\n",
      "GTArea < c\n",
      "0, otherwise\n",
      "where IoU is the Intersection-Over-Union score of the s ×s\n",
      "label block with the ground-truth bounding box, GTArea is the\n",
      "area of the re-scaled ground-truth bounding box, a is typically\n",
      "5, b is 64, and c is 90. If multiple ground-truth bounding boxes\n",
      "overlap with a pixel, FocusPixels ( l = 1) are given precedence.\n",
      "Since our network is trained on 512 ×512 pixel chips, the ratio\n",
      "between the positive and negative pixels is around 10, so we do not\n",
      "perform any re-weighting for the loss. Note that during multi-scale\n",
      "training, the same ground-truth could generate a label of 1, 0 or -1 \n",
      "#@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@#\n",
      "\n",
      "Since our network is trained on 512 ×512 pixel chips, the ratio\n",
      "between the positive and negative pixels is around 10, so we do not\n",
      "perform any re-weighting for the loss. Note that during multi-scale\n",
      "training, the same ground-truth could generate a label of 1, 0 or -1\n",
      "depending on how much it has been scaled. The labeling scheme\n",
      "is visually depicted in Fig 9. For training the network, we add two\n",
      "convolutional layers (3×3 and 1 ×1) with ReLU non-linearity on\n",
      "top of the conv5 feature-map. Finally, we have a binary softmax\n",
      "classiﬁer to predict FocusPixels, shown in Fig 11.\n",
      "3.4.2 FocusChip Generation\n",
      "Armed with the capability of estimating the foreground probability\n",
      "at every pixel, we now turn our attention to obtain rectangular\n",
      "sub-regions, or FocusChips, for further processing with a CNN.\n",
      "During inference, we use a parameter, t, to mark the pixels, P,\n",
      "whose foreground probability is greater than t as FocusPixels.\n",
      "Consequently, a higher value of t will lead to a smaller number of \n",
      "#@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@#\n",
      "\n",
      "sub-regions, or FocusChips, for further processing with a CNN.\n",
      "During inference, we use a parameter, t, to mark the pixels, P,\n",
      "whose foreground probability is greater than t as FocusPixels.\n",
      "Consequently, a higher value of t will lead to a smaller number of\n",
      "FocusPixel for further processing. Therefore, t controls the speed-\n",
      "up and can be set with respect to the desired speed-accuracy trade-\n",
      "off. The thresholding with t generates a set of connected compo-\n",
      "nents S, which are dilated with a d ×d sized-ﬁlter to increase\n",
      "the amount of required contextual information for recognition.\n",
      "As a result of dilation, previously disconnected components can\n",
      "\f",
      "10\n",
      "Positive\n",
      "Negative\n",
      "Don’t Care\n",
      "GT box\n",
      "Legend\n",
      "(a) Image\n",
      " (b) Scale 1\n",
      "(c) Scale 2\n",
      "(d) Scale 3\n",
      "Fig. 9: The ﬁgure illustrates how FocusPixels are assigned at multiple scales of an image. At scale 1 (b), the smallest two elephants \n",
      "#@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@#\n",
      "\n",
      "As a result of dilation, previously disconnected components can\n",
      "\f",
      "10\n",
      "Positive\n",
      "Negative\n",
      "Don’t Care\n",
      "GT box\n",
      "Legend\n",
      "(a) Image\n",
      " (b) Scale 1\n",
      "(c) Scale 2\n",
      "(d) Scale 3\n",
      "Fig. 9: The ﬁgure illustrates how FocusPixels are assigned at multiple scales of an image. At scale 1 (b), the smallest two elephants\n",
      "generate FocusPixels, the largest one is marked as background and the one on the left is ignored during training to avoid penalizing the\n",
      "network for borderline cases (see Sec. 3.4.1 for assignment details). The labelling changes at scales 2 and 3 as the objects occupy more\n",
      "pixels. For example, only the smallest elephant would generate FocusPixels at scale 2 and the largest two elephants would generate\n",
      "negative labels.\n",
      "(a) (b) (c) (d)\n",
      "Fig. 10: Pruning detections while FocusStacking. (a) Original Image (b) The predicted FocusPixels and the generated FocusChip (c)\n",
      "Detection output by the network (d) Final detections for the FocusChip after pruning. \n",
      "#@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@#\n",
      "\n",
      "negative labels.\n",
      "(a) (b) (c) (d)\n",
      "Fig. 10: Pruning detections while FocusStacking. (a) Original Image (b) The predicted FocusPixels and the generated FocusChip (c)\n",
      "Detection output by the network (d) Final detections for the FocusChip after pruning.\n",
      "form a new connection. Such components are merged to obtain the\n",
      "ﬁnal set of connected components. Finally, we generate chips C\n",
      "that enclose the set of the aforementioned connected components.\n",
      "Note that the chips containing two connected components could\n",
      "overlap. As a result, these chips are merged with each other\n",
      "and replaced with their enclosing bounding-boxes in C. Some\n",
      "connected components could be extremely small, and potentially\n",
      "lack the required contextual information for accurate recognition.\n",
      "Many small chips also increase fragmentation which results in a\n",
      "wide range of chip sizes. This makes batch-inference inefﬁcient.\n",
      "To avoid these problems, we ensure that the height and width of a \n",
      "#@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@#\n",
      "\n",
      "lack the required contextual information for accurate recognition.\n",
      "Many small chips also increase fragmentation which results in a\n",
      "wide range of chip sizes. This makes batch-inference inefﬁcient.\n",
      "To avoid these problems, we ensure that the height and width of a\n",
      "chip is greater than a minimum size k. This process is described\n",
      "in Algorithm 2. With the help of the identiﬁed FocusChips, we\n",
      "perform multi-scale inference on an image pyramid while focusing\n",
      "on regions that are more likely to contain objects.\n",
      "3.4.3 Focus Stacking for Object Detection\n",
      "One issue with such cascaded multi-scale inference is that some\n",
      "detections at the boundary of the chips can be generated for\n",
      "cropped objects which were originally large. At the next scale, due\n",
      "to cropping, they could become small and generate false positives,\n",
      "such as the detections for the horse and the horse rider on the right,\n",
      "shown in Fig 10 (c). To alleviate this effect, Step 2 in Algorithm \n",
      "#@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@#\n",
      "\n",
      "cropped objects which were originally large. At the next scale, due\n",
      "to cropping, they could become small and generate false positives,\n",
      "such as the detections for the horse and the horse rider on the right,\n",
      "shown in Fig 10 (c). To alleviate this effect, Step 2 in Algorithm\n",
      "2 is very important. Note that when we dilate the map Pand\n",
      "generate chips, this ensures that no interesting object at the next\n",
      "scale would be observed at the boundaries of the chip (unless\n",
      "Algorithm 2:FocusChip Generator\n",
      "Input : Predictions for feature map P, threshold t,\n",
      "dilation constant d, minimum size of chip k\n",
      "Output: Chips C\n",
      "1 Transform Pinto a binary map using the threshold t\n",
      "2 Dilate Pwith a d ×d ﬁlter\n",
      "3 Obtain a set of connected components Sfrom P\n",
      "4 Generate enclosing chips Cof size > kfor each\n",
      "component in S\n",
      "5 Merge chips Cif they overlap\n",
      "6 return Chips C\n",
      "the chip shares a border with the image boundary). Otherwise, it\n",
      "would be enclosed by the chip, as these are generated around the \n",
      "#@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@#\n",
      "\n",
      "3 Obtain a set of connected components Sfrom P\n",
      "4 Generate enclosing chips Cof size > kfor each\n",
      "component in S\n",
      "5 Merge chips Cif they overlap\n",
      "6 return Chips C\n",
      "the chip shares a border with the image boundary). Otherwise, it\n",
      "would be enclosed by the chip, as these are generated around the\n",
      "dilated maps. Therefore, if a detection in the zoomed-in chip is\n",
      "observed at the boundary, we discard it, even if it is within valid\n",
      "SNIP ranges, such as the horse rider eliminated in Fig 10 (d).\n",
      "There are some corner cases when the detection is at the\n",
      "boundary (or boundaries x, y) of the image. If the chip shares\n",
      "one boundary with the image, we still check if the other side of\n",
      "the detection is completely enclosed inside or not. If it is not, we\n",
      "discard it, else we keep it. In another case, if the chip shares both\n",
      "the sides with the image boundary and so does the detection, then\n",
      "we keep the detection.\n",
      "\f",
      "11 \n",
      "#@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@#\n",
      "\n",
      "one boundary with the image, we still check if the other side of\n",
      "the detection is completely enclosed inside or not. If it is not, we\n",
      "discard it, else we keep it. In another case, if the chip shares both\n",
      "the sides with the image boundary and so does the detection, then\n",
      "we keep the detection.\n",
      "\f",
      "11\n",
      "Fig. 11: The ﬁgure illustrates how AutoFocus detects a person and a racket in an image. The green borders and arrows are for inference\n",
      "at the original resolution. The blue borders and arrows are shown when inference is performed inside FocusChips. In the ﬁrst iteration,\n",
      "the network detects the person and also generates a heat-map to mark regions containing small objects. This is depicted in the white/grey\n",
      "map - it is used to generate FocusChips. In the next iteration, the detector is then applied inside FocusChips only. Inside FocusChips,\n",
      "there could be detections for the cropped object present at the larger resolution. Such detections are pruned and ﬁnally, valid detections \n",
      "#@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@#\n",
      "\n",
      "map - it is used to generate FocusChips. In the next iteration, the detector is then applied inside FocusChips only. Inside FocusChips,\n",
      "there could be detections for the cropped object present at the larger resolution. Such detections are pruned and ﬁnally, valid detections\n",
      "are stacked across multiple scales.\n",
      "Once valid detections from each scale are obtained using the\n",
      "above rules, we merge detections from all the scales by projecting\n",
      "them to the image co-ordinates after applying appropriate scaling\n",
      "and translation. Finally, Non-Maximum Suppression is applied to\n",
      "aggregate the detections. The network architecture and an example\n",
      "of multi-scale inference and focus stacking is shown in Fig 11.\n",
      "3.5 Putting it all together\n",
      "Here we summarize the proposed concepts and considerations\n",
      "leading up to the ﬁnal scale-normalized object-detection paradigm\n",
      "to facilitate clear dissemination. First, we introduced the concept\n",
      "of scale-normalization for image-pyramids (SNIP) to effectively \n",
      "#@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@#\n",
      "\n",
      "3.5 Putting it all together\n",
      "Here we summarize the proposed concepts and considerations\n",
      "leading up to the ﬁnal scale-normalized object-detection paradigm\n",
      "to facilitate clear dissemination. First, we introduced the concept\n",
      "of scale-normalization for image-pyramids (SNIP) to effectively\n",
      "tackle the adverse effects of extreme-scale objects during train-\n",
      "ing and put forward concrete guidelines for setting it’s design\n",
      "parameters. This was followed by an efﬁcient scale-normalized\n",
      "spatial sub-sampling mechanism (SNIPER) to reduce the ad-\n",
      "ditional computational cost involved in processing the multi-\n",
      "scale image-pyramid during training. We discussed the use of\n",
      "image content for SNIPER’s spatial sub-sampling, described the\n",
      "positive/negative chip-sampling and their labeling scheme, and\n",
      "the additional beneﬁts of SNIPER over SNIP with the help of\n",
      "increased batch-size and batch-normalization [60] for the training\n",
      "phase. Lastly, we modeled the active foveal-vision in humans in \n",
      "#@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@#\n",
      "\n",
      "positive/negative chip-sampling and their labeling scheme, and\n",
      "the additional beneﬁts of SNIPER over SNIP with the help of\n",
      "increased batch-size and batch-normalization [60] for the training\n",
      "phase. Lastly, we modeled the active foveal-vision in humans in\n",
      "the form of AutoFocus that processes a multi-scale image-pyramid\n",
      "in a coarse-to-ﬁne manner to reduce run-time computational cost\n",
      "during inference. We discussed the concept of FocusPixels that are\n",
      "used as a proxy for ﬁnding interesting regions which are the only\n",
      "spatial sub-regions where the detector needs to be applied during\n",
      "inference. This leads to the ﬁnal system, which tackles scale-\n",
      "variation and can be efﬁciently trained and tested on multi-scale\n",
      "image-pyramids. The proposed scale-normalization approach for\n",
      "object-detection makes several changes to the existing pipeline\n",
      "both during training and inference stages. Therefore, in this sec-\n",
      "tion, we employ the COCO dataset to carry out extensive ablation \n",
      "#@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@#\n",
      "\n",
      "image-pyramids. The proposed scale-normalization approach for\n",
      "object-detection makes several changes to the existing pipeline\n",
      "both during training and inference stages. Therefore, in this sec-\n",
      "tion, we employ the COCO dataset to carry out extensive ablation\n",
      "studies to clearly reveal the effects of different modules, namely\n",
      "SNIP, SNIPER, and AutoFocus which have been proposed. The\n",
      "comparisons with the other approaches follow standard protocol\n",
      "and use 123,000 images from the training and 20,288 images in\n",
      "test-dev set of COCO for training and evaluation. Since recall for\n",
      "proposals is not provided by the evaluation server on COCO, we\n",
      "train on 118,000 images and report recall on the remaining 5,000\n",
      "images (commonly referred to as minival set, or the 2017 test-dev\n",
      "set). Unless speciﬁcally mentioned, the area of small objects is\n",
      "less than 32x32, medium objects range from 32x32 to 96x96 and\n",
      "large objects are greater than 96x96 pixels.\n",
      "4 E XPERIMENTAL ANALYSIS\n",
      "4.1 Training Details \n",
      "#@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@#\n",
      "\n",
      "images (commonly referred to as minival set, or the 2017 test-dev\n",
      "set). Unless speciﬁcally mentioned, the area of small objects is\n",
      "less than 32x32, medium objects range from 32x32 to 96x96 and\n",
      "large objects are greater than 96x96 pixels.\n",
      "4 E XPERIMENTAL ANALYSIS\n",
      "4.1 Training Details\n",
      "We use 3 resolutions of (480, 800), (800, 1200), and (1400, 2000)\n",
      "pixels for training our detectors. The ﬁrst value is for the shorter\n",
      "side of the image and the second one is the limit on the maximum\n",
      "side size. The valid ranges are set to ( 0,802), ( 322, 1502), and\n",
      "(1202, ∞) which ensure that at least 5 to 15 convolutional features\n",
      "are observed in the coarsest convolutional layer of the network.\n",
      "The training of the detectors is performed for 7 epochs. Except\n",
      "when RPN is trained separately, we use a shorter training period\n",
      "of 6 epochs.\n",
      "We start the training with a warmup learning rate of 0.0005\n",
      "for 1000 iterations. Since we use the efﬁcient resampling scheme, \n",
      "#@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@#\n",
      "\n",
      "The training of the detectors is performed for 7 epochs. Except\n",
      "when RPN is trained separately, we use a shorter training period\n",
      "of 6 epochs.\n",
      "We start the training with a warmup learning rate of 0.0005\n",
      "for 1000 iterations. Since we use the efﬁcient resampling scheme,\n",
      "we can use a batch size of 128 chips for 512x512 pixels and\n",
      "a base learning rate of 0.015. When ablation experiments are\n",
      "performed for scale normalization, we use a batch size of 8 (1\n",
      "per GPU) and a learning rate of 0.005. We use mixed-precision\n",
      "training as described in [61]. To this end, we re-scale weight-\n",
      "decay by 100, drop the learning rate by 100, and re-scale gradients\n",
      "by 100. This ensures that we can train with activations of half-\n",
      "precision (and hence ∼2x larger batch size) without any drop of\n",
      "accuracy. FP32 weights are used for the ﬁrst convolution layer, last\n",
      "convolution layer in RPN (classiﬁcation and regression), and the\n",
      "fully connected layers in Faster-RCNN. We drop the learning rate \n",
      "#@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@#\n",
      "\n",
      "precision (and hence ∼2x larger batch size) without any drop of\n",
      "accuracy. FP32 weights are used for the ﬁrst convolution layer, last\n",
      "convolution layer in RPN (classiﬁcation and regression), and the\n",
      "fully connected layers in Faster-RCNN. We drop the learning rate\n",
      "after 5.33 epochs (except when RPN is trained separately where\n",
      "we drop the learning rate after 4.33 epochs). Image ﬂipping is used\n",
      "as a data-augmentation technique.\n",
      "\f",
      "12\n",
      "TABLE 2: MS denotes multi-scale. Single scale is (800,1200).\n",
      "R-FCN detector with ResNet-50 (as described in Section 4).\n",
      "Method AP APS APM APL\n",
      "Single scale 34.5 16.3 37.2 47.6\n",
      "MS Test 35.9 19.5 37.3 48.5\n",
      "MS Train/Test 35.6 19.5 37.5 47.3\n",
      "SNIP 37.8 21.4 40.4 50.1\n",
      "As mentioned in Section 3.3.3, an RPN is deployed for\n",
      "negative chip sampling in SNIPER. We train this RPN only for 2\n",
      "epochs with a ﬁxed learning rate of 0.015 without any step-down.\n",
      "Therefore, it requires less than 20% of the total training time. \n",
      "#@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@#\n",
      "\n",
      "SNIP 37.8 21.4 40.4 50.1\n",
      "As mentioned in Section 3.3.3, an RPN is deployed for\n",
      "negative chip sampling in SNIPER. We train this RPN only for 2\n",
      "epochs with a ﬁxed learning rate of 0.015 without any step-down.\n",
      "Therefore, it requires less than 20% of the total training time.\n",
      "RPN proposals are extracted from all scales. Note that inference\n",
      "takes 1/3 the time for a full forward-backward pass and we do not\n",
      "perform any ﬂipping for extracting proposals. Hence, this process\n",
      "is also efﬁcient.\n",
      "4.2 Effectiveness of SNIP against Scale Variation\n",
      "In this section, we carry different experiments to understand the\n",
      "behavior of SNIP under the variations of scale-range, object-\n",
      "detection architecture and to show the beneﬁts of employing\n",
      "SNIP with other popular architectures. In order to disentangle\n",
      "and clearly understand the tolerance against scale-variation offered\n",
      "by SNIP in the two-stage object-detection pipelines, we evaluate\n",
      "the region-proposal and classiﬁcation modules of the detection \n",
      "#@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@#\n",
      "\n",
      "SNIP with other popular architectures. In order to disentangle\n",
      "and clearly understand the tolerance against scale-variation offered\n",
      "by SNIP in the two-stage object-detection pipelines, we evaluate\n",
      "the region-proposal and classiﬁcation modules of the detection\n",
      "network separately.\n",
      "4.2.1 SNIP for RCN improvements\n",
      "In this ablation study, we use a single-scale proposal generation\n",
      "that is common across all the three scales of the multi-scale\n",
      "pyramid to generate the proposals. The generated proposals are\n",
      "used to evaluate the performance of SNIP on the RCN only, under\n",
      "the same settings as described in Section 3.2. This study aims\n",
      "at demonstrating the beneﬁts of SNIP over the vanilla Multi-Scale\n",
      "Training/Testing pipeline. Therefore, we compare the performance\n",
      "of single-scale train/test, multi-scale test and multi-scale train/test\n",
      "protocols against SNIP in Table 2. For a fair comparison, we\n",
      "use the best possible validity ranges at each scale for all the \n",
      "#@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@#\n",
      "\n",
      "Training/Testing pipeline. Therefore, we compare the performance\n",
      "of single-scale train/test, multi-scale test and multi-scale train/test\n",
      "protocols against SNIP in Table 2. For a fair comparison, we\n",
      "use the best possible validity ranges at each scale for all the\n",
      "protocols where multi-scale testing is performed. As expected,\n",
      "multi-scale testing yields an improvement of 1.4% over single-\n",
      "scale train/test protocol. Naturally, we would expect that the multi-\n",
      "scale train/test protocol would improve it even further because\n",
      "multi-scale samples are used during training as well. However, it\n",
      "ended up reducing the improvement to only 1.1% which clearly\n",
      "demonstrates that the inclusion of large objects (especially in\n",
      "the 1400×2000 resolution) during multi-scale training adversely\n",
      "affects the training of the RCN. It happens due to the inability of\n",
      "the effective network receptive ﬁeld to correctly classify extremely\n",
      "blown-up objects in up-scaled images. Finally, SNIP improved the \n",
      "#@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@#\n",
      "\n",
      "the 1400×2000 resolution) during multi-scale training adversely\n",
      "affects the training of the RCN. It happens due to the inability of\n",
      "the effective network receptive ﬁeld to correctly classify extremely\n",
      "blown-up objects in up-scaled images. Finally, SNIP improved the\n",
      "performance by 3.3% and 1.9% over single-scale train/test and\n",
      "multi-scale test protocols, respectively. This experiment clearly\n",
      "demonstrates the beneﬁts of using SNIP during training to effec-\n",
      "tively avoid presenting large scale-variation to the RCN.\n",
      "4.2.2 SNIP for RPN improvements\n",
      "Now we turn our attention to the region-proposal network of the\n",
      "object detection pipeline. Before proceeding with the ablations\n",
      "studies with SNIP, note that the recall at 50% overlap is the most\n",
      "important performance metric for object proposals; it’s because\n",
      "TABLE 3: For individual ranges (like 0-25 etc.) recall at 50%\n",
      "overlap is reported because minor localization errors can be ﬁxed \n",
      "#@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@#\n",
      "\n",
      "studies with SNIP, note that the recall at 50% overlap is the most\n",
      "important performance metric for object proposals; it’s because\n",
      "TABLE 3: For individual ranges (like 0-25 etc.) recall at 50%\n",
      "overlap is reported because minor localization errors can be ﬁxed\n",
      "in the second stage. ResNet-50 is used as the backbone. Recall is\n",
      "for 900 proposals, as top 300 are taken from each scale.\n",
      "Method AR AR50 AR75 0-25 25-50 50-100\n",
      "Baseline 61.3 89.2 69.8 68.1 91.0 96.7\n",
      "+ SNIP 64.0 92.1 74.7 74.4 95.1 98.0\n",
      "TABLE 4: The effect of SNIP on RCN and RPN\n",
      "Method Backbone RPN SNIP RCN SNIP AP\n",
      "D-R-FCN DPN-98\n",
      "\u0017 \u0017 41.2\n",
      "\u0017 \u0013 44.2\n",
      "\u0013 \u0013 44.7\n",
      "Faster-RCNN ResNet-101\n",
      "\u0017 \u0017 42.6\n",
      "\u0013 \u0017 43.1\n",
      "\u0013 \u0013 44.4\n",
      "bounding box regression can correct minor localization errors,\n",
      "but an uncovered object by all the proposals will certainly result\n",
      "in false negative. Since recall at 50% overlap for the objects\n",
      ">100 pixel in size is already close to 100%, further improvement\n",
      "wouldn’t lead to any signiﬁcant overall improvements. Improving \n",
      "#@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@#\n",
      "\n",
      "but an uncovered object by all the proposals will certainly result\n",
      "in false negative. Since recall at 50% overlap for the objects\n",
      ">100 pixel in size is already close to 100%, further improvement\n",
      "wouldn’t lead to any signiﬁcant overall improvements. Improving\n",
      "the recall on small objects, however, would lead to more overall\n",
      "gains. In order to demonstrate the beneﬁts of SNIP training on\n",
      "differently sized objects, we show the improvements for a ResNet-\n",
      "50 RPN network in Table 3. First, note that SNIP improved the\n",
      "overall recall at 50% overlap by 2.9% and 6.3% for objects smaller\n",
      "than 25 pixels. If we train our RPN without SNIP, mAP drops by\n",
      "1.1% on small objects and 0.5% overall. Note that AP of large\n",
      "objects is not affected as we still use the classiﬁcation model\n",
      "trained with SNIP. We also perform an ablation study with stronger\n",
      "backbones like DPN-98 and detectors like Faster-RCNN which are\n",
      "shown in Table 4.\n",
      "4.3 Efﬁcient Training with SNIPER \n",
      "#@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@#\n",
      "\n",
      "objects is not affected as we still use the classiﬁcation model\n",
      "trained with SNIP. We also perform an ablation study with stronger\n",
      "backbones like DPN-98 and detectors like Faster-RCNN which are\n",
      "shown in Table 4.\n",
      "4.3 Efﬁcient Training with SNIPER\n",
      "While SNIP improves training on multi-scale image pyramid,\n",
      "it comes at a computational cost. In this section, we carry out\n",
      "ablation studies pertaining to the efﬁcient re-sampling scheme for\n",
      "training with SNIP (or SNIPER). As SNIPER does not use all the\n",
      "training data and uses RPN to generate chips for training, we have\n",
      "to ensure that RPN recall is good. Moreover, we lose a signiﬁcant\n",
      "amount of background samples during training, therefore, it’s\n",
      "important to assess the effect of negative chip mining. The ﬁnal\n",
      "goal of design parameters is to ensure that SNIPER’s performance\n",
      "matches the SNIP baseline, which trains on the entire image\n",
      "pyramid, while obtaining a signiﬁcant speedup. In the following \n",
      "#@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@#\n",
      "\n",
      "important to assess the effect of negative chip mining. The ﬁnal\n",
      "goal of design parameters is to ensure that SNIPER’s performance\n",
      "matches the SNIP baseline, which trains on the entire image\n",
      "pyramid, while obtaining a signiﬁcant speedup. In the following\n",
      "sub-sections, we focus on different analyses pertaining to the\n",
      "design parameters of SNIPER to achieve the aforementioned goal.\n",
      "4.3.1 SNIPER Recall Analysis\n",
      "Since the positive chip-sampling covers all the ground truth\n",
      "samples, we posit that it’s sufﬁcient to train on just the positive\n",
      "samples for generating proposals and still maintain a high recall.\n",
      "Indeed, we observe that the recall (averaged over multiple overlap\n",
      "thresholds 0.5:0.05:0.95) for RPN is unaffected w.r.t. negative\n",
      "sampling (Table 5) because recall doesn’t account for false posi-\n",
      "tives. The aforementioned intuitive reasoning and empirical results\n",
      "bolster SNIPER’s strategy of employing an RPN, which is trained\n",
      "\f",
      "13\n",
      "TABLE 5: We plot the recall for SNIPER with and without \n",
      "#@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@#\n",
      "\n",
      "sampling (Table 5) because recall doesn’t account for false posi-\n",
      "tives. The aforementioned intuitive reasoning and empirical results\n",
      "bolster SNIPER’s strategy of employing an RPN, which is trained\n",
      "\f",
      "13\n",
      "TABLE 5: We plot the recall for SNIPER with and without\n",
      "negatives. Surprisingly, recall is not effected by negative chip\n",
      "sampling\n",
      "NEG. AR AR50 AR75 0-25 25-50 50-100 100-300\n",
      "\u0013 65.4 93.2 76.9 41.3 65.8 74.5 77.8\n",
      "\u0017 65.4 93.2 77.6 40.8 65.7 74.7 78.3\n",
      "TABLE 6: The effect training on 2 scales (1.667 and max size of\n",
      "512). We also show the impact in performance when no negative\n",
      "mining is performed. A ResNet-101 backbone is used.\n",
      "Method AP AP50 AP75 APS APM APL\n",
      "SNIPER 46.1 67.0 51.6 29.6 48.9 58.1\n",
      "No Neg. 43.4 62.8 48.8 27.4 45.2 56.2\n",
      "2 Scales 43.3 63.7 48.6 27.1 44.7 56.1\n",
      "on positive samples only, for negative chip-sampling. However,\n",
      "mAP score for detection depends on false-positives, as shown\n",
      "in Sec. 3.3.3, hence negative sampling, discussed next, plays an\n",
      "important role as well. \n",
      "#@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@#\n",
      "\n",
      "No Neg. 43.4 62.8 48.8 27.4 45.2 56.2\n",
      "2 Scales 43.3 63.7 48.6 27.1 44.7 56.1\n",
      "on positive samples only, for negative chip-sampling. However,\n",
      "mAP score for detection depends on false-positives, as shown\n",
      "in Sec. 3.3.3, hence negative sampling, discussed next, plays an\n",
      "important role as well.\n",
      "4.3.2 Effect of Negative Chip Mining on SNIPER\n",
      "Just like any other object-detection system, SNIPER also employs\n",
      "negative chip mining to reduce the false-positive rate. Addition-\n",
      "ally, SNIPER also aims at speeding-up the training by skipping\n",
      "the easy regions inside the image, which are obtained from an\n",
      "RPN trained with a short learning schedule, Sec. 3.3.3. Inclusion\n",
      "of negative samples that are similar in appearance to positive\n",
      "instances is a well-known technique to reduce the false-positive\n",
      "rate and helps to improve the overall mAP, which depends on\n",
      "both the recall and precision. To evaluate the effectiveness of our\n",
      "negative mining approach, we compare SNIPER’s mAP score with \n",
      "#@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@#\n",
      "\n",
      "instances is a well-known technique to reduce the false-positive\n",
      "rate and helps to improve the overall mAP, which depends on\n",
      "both the recall and precision. To evaluate the effectiveness of our\n",
      "negative mining approach, we compare SNIPER’s mAP score with\n",
      "a variant that only uses positive chips during training, Table 6,\n",
      "while keeping other parameters the same. The proposed negative\n",
      "chip mining approach noticeably improves AP scores for all\n",
      "localization thresholds and object sizes and improves the mAP\n",
      "score from 43.4 to 46.1.\n",
      "4.3.3 Effect of Multi-Scale Training on SNIPER\n",
      "In order to illustrate the beneﬁts of multi-scale training using\n",
      "SNIPER, we reduce the number of scales from 3 to 2 by dropping\n",
      "the highest resolution scale and trained with SNIPER. This variant\n",
      "is compared with standard SNIPER training that employs all 3\n",
      "scales for training and the results are compared in Table 6. We\n",
      "can see that the reduction in the number of scales signiﬁcantly de- \n",
      "#@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@#\n",
      "\n",
      "the highest resolution scale and trained with SNIPER. This variant\n",
      "is compared with standard SNIPER training that employs all 3\n",
      "scales for training and the results are compared in Table 6. We\n",
      "can see that the reduction in the number of scales signiﬁcantly de-\n",
      "creased the performance consistently across all evaluation metrics.\n",
      "4.3.4 Comparison with training on Full Image Pyramids\n",
      "Since SNIPER reduces both the memory and computational foot-\n",
      "print while processing a multi-scale image pyramid, it affords\n",
      "increased batch-size and effective batch-normalization [60] during\n",
      "training, which was otherwise not possible with SNIP on commod-\n",
      "ity GPU cards. Therefore, in order to compare SNIPER with SNIP,\n",
      "we turn-off batch-normalization during SNIPER training and show\n",
      "that it achieves matching results with SNIP, in Table 7. With\n",
      "batch-normalization, SNIPER signiﬁcantly outperforms SNIP in\n",
      "all metrics and obtains an mAP of 46.1%. This result improves to \n",
      "#@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@#\n",
      "\n",
      "we turn-off batch-normalization during SNIPER training and show\n",
      "that it achieves matching results with SNIP, in Table 7. With\n",
      "batch-normalization, SNIPER signiﬁcantly outperforms SNIP in\n",
      "all metrics and obtains an mAP of 46.1%. This result improves to\n",
      "46.8% if we pre-train the detector on the OpenImagesV4 dataset.\n",
      "Adding an instance segmentation head and training the detection\n",
      "TABLE 7: We observe that SNIPER matches the performance\n",
      "even after reducing the pixels processed by 3.5 times.\n",
      "Method AP AP50 AP75 APS APM APL\n",
      "SNIP 43.6 65.2 48.8 26.4 46.5 55.8\n",
      "SNIPER 43.5 65.0 48.6 26.1 46.3 56.0\n",
      "TABLE 8: We highlight the importance of image pyramids even\n",
      "with lightweight backbones, where we see a 12% gain in per-\n",
      "formance. Pre-training with additional data and multi-tasking with\n",
      "instance segmentation brings a 1.5% improvement in performance.\n",
      "Method Backbone AP\n",
      "SSD MobileNet-v2 22.1\n",
      "SNIPER MobileNet-v2 34.5\n",
      "SNIPER ResNet-101 46.1\n",
      "SNIPER + OpenImages ResNet-101 46.8 \n",
      "#@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@#\n",
      "\n",
      "formance. Pre-training with additional data and multi-tasking with\n",
      "instance segmentation brings a 1.5% improvement in performance.\n",
      "Method Backbone AP\n",
      "SSD MobileNet-v2 22.1\n",
      "SNIPER MobileNet-v2 34.5\n",
      "SNIPER ResNet-101 46.1\n",
      "SNIPER + OpenImages ResNet-101 46.8\n",
      "SNIPER + OpenImages + Mask Training ResNet-101 47.6\n",
      "network along with it further improves the performance to 47.6%.\n",
      "We also show results for Faster-RCNN trained with MobileNetV2.\n",
      "It obtains an mAP of 34.1% compared to the SSDLite [62] version\n",
      "which obtained 22.1%. This again highlights the importance of\n",
      "image pyramids (and SNIPER training) as we can improve the\n",
      "performance of the detector by 12%, Table 8. Not only is SNIPER\n",
      "more accurate, it is also 3×faster compared to SNIP during\n",
      "training. It only takes 14 hours for end-to-end training on a 8x\n",
      "V100 GPU node with a Faster-RCNN detector with ResNet-101\n",
      "backbone. It is worth noting that we train on 3 scales of an image\n",
      "pyramid (max size of 512, 1.667 and 3). Training RPN is much \n",
      "#@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@#\n",
      "\n",
      "training. It only takes 14 hours for end-to-end training on a 8x\n",
      "V100 GPU node with a Faster-RCNN detector with ResNet-101\n",
      "backbone. It is worth noting that we train on 3 scales of an image\n",
      "pyramid (max size of 512, 1.667 and 3). Training RPN is much\n",
      "more efﬁcient and it only takes 2 hours.\n",
      "4.4 Efﬁcient Inference with AutoFocus\n",
      "While SNIPER improves the efﬁciency of training by skipping\n",
      "“easy” regions, it is not directly applicable during inference as it\n",
      "requires ground-truth information for chip sampling. As discussed\n",
      "in 3.4, AutoFocus extends the active sub-sampling concept to the\n",
      "inference phase by predicting “FocusPixels” and generating “Fo-\n",
      "cusChips” from them. In this section, we empirically – evaluate\n",
      "the underlying hypotheses behind AutoFocus and the quality of\n",
      "the estimated FocusPixels/Chips w.r.t. the ground-truth, compare\n",
      "AutoFocus with SNIPER, and study the Speed-Accuracy trade-off\n",
      "w.r.t. design parameters.\n",
      "0 64 128 256 512\n",
      "Min Chip Size ( k)\n",
      "0X\n",
      "2X\n",
      "4X\n",
      "6X\n",
      "8X\n",
      "10X \n",
      "#@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@#\n",
      "\n",
      "the underlying hypotheses behind AutoFocus and the quality of\n",
      "the estimated FocusPixels/Chips w.r.t. the ground-truth, compare\n",
      "AutoFocus with SNIPER, and study the Speed-Accuracy trade-off\n",
      "w.r.t. design parameters.\n",
      "0 64 128 256 512\n",
      "Min Chip Size ( k)\n",
      "0X\n",
      "2X\n",
      "4X\n",
      "6X\n",
      "8X\n",
      "10X\n",
      "Pixels in S1+S2+S3\n",
      "Pixels in FocusChips\n",
      "0.3 0.4 0.8 1.4\n",
      "Milion Pixels Processed\n",
      "Fig. 12: Upper-bound on the speed-up using FocusChips generated\n",
      "from optimal FocusPixels.\n",
      "\f",
      "14\n",
      "0.2 0.4 0.6\n",
      "FocusPixel Area / Image Area\n",
      "0.0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "1.0\n",
      "Recall\n",
      "Focus Pixels\n",
      "Scale 1 - 480x512 (GTs)\n",
      "Scale 2 - 800x1280 (GTs)\n",
      "(a)\n",
      "0.2 0.4 0.6\n",
      "FocusPixel Area / Image Area\n",
      "0.0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "1.0\n",
      "Recall\n",
      "Focus Pixels\n",
      "Scale 1 - 480x800 (Dets)\n",
      "Scale 2 - 800x1280 (Dets) (b)\n",
      "0.0 0.2 0.4 0.6 0.8 1.0\n",
      "FocusChip Area / Image Area\n",
      "0.0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "1.0\n",
      "Recall\n",
      "Focus Chips\n",
      "Scale 1 - 480x512 (GTs)\n",
      "Scale 2 - 800x1280 (GTs) (c)\n",
      "0.0 0.2 0.4 0.6 0.8 1.0\n",
      "FocusChip Area / Image Area\n",
      "0.0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "1.0\n",
      "Recall\n",
      "Focus Chips\n",
      "Scale 1 - 480x512 (Dets) \n",
      "#@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@#\n",
      "\n",
      "Scale 2 - 800x1280 (Dets) (b)\n",
      "0.0 0.2 0.4 0.6 0.8 1.0\n",
      "FocusChip Area / Image Area\n",
      "0.0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "1.0\n",
      "Recall\n",
      "Focus Chips\n",
      "Scale 1 - 480x512 (GTs)\n",
      "Scale 2 - 800x1280 (GTs) (c)\n",
      "0.0 0.2 0.4 0.6 0.8 1.0\n",
      "FocusChip Area / Image Area\n",
      "0.0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "1.0\n",
      "Recall\n",
      "Focus Chips\n",
      "Scale 1 - 480x512 (Dets)\n",
      "Scale 2 - 800x1280 (Dets) (d)\n",
      "Fig. 13: Quality of the FocusPixels and FocusChips. The x-axis represents the ratio of the area of FocusPixels or FocusChips to that of\n",
      "the image. The y-axis changes as follows, (a) FocusPixel recall is computed based on the GT boxes (b) FocusPixel recall is computed\n",
      "using the conﬁdent detections (c) FocusChip recall is computed based on the GT boxes (d) FocusChip recall is computed based on the\n",
      "conﬁdent detections.\n",
      "4.4.1 AutoFocus Hypothesis Testing\n",
      "The core hypothesis behind AutoFocus is the low percentage of\n",
      "the FocusPixels in natural images, especially in high-resolution\n",
      "images. To investigate this hypothesis, here, we report the per- \n",
      "#@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@#\n",
      "\n",
      "conﬁdent detections.\n",
      "4.4.1 AutoFocus Hypothesis Testing\n",
      "The core hypothesis behind AutoFocus is the low percentage of\n",
      "the FocusPixels in natural images, especially in high-resolution\n",
      "images. To investigate this hypothesis, here, we report the per-\n",
      "centage of the FocusPixels at different scales for the validation\n",
      "set of the COCO dataset based on ground-truth annotations. In\n",
      "high-resolution images (scale 3), the percentage of FocusPixels is\n",
      "very low (i.e. ∼4%). Therefore, ideally, a very small part of the\n",
      "image needs to be processed at high resolution. Since the image\n",
      "is up-sampled, the FocusPixels projected on the image occupy an\n",
      "area of 632 pixels on average (the highest resolution images have\n",
      "an area of 16022 pixels on average). At lower scales (like scale\n",
      "2), although the percentage of FocusPixels increases to ∼11%,\n",
      "their projections only occupy an area of 1022 pixels on average\n",
      "(each image at this scale has an average area of9402 pixels). After \n",
      "#@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@#\n",
      "\n",
      "an area of 16022 pixels on average). At lower scales (like scale\n",
      "2), although the percentage of FocusPixels increases to ∼11%,\n",
      "their projections only occupy an area of 1022 pixels on average\n",
      "(each image at this scale has an average area of9402 pixels). After\n",
      "dilating FocusPixels with a kernel of size 3 ×3, their percentages\n",
      "at scale 3 and scale 2 change to 7% and 18% respectively.\n",
      "Using the chip generation algorithm, for a given minimum chip\n",
      "size (like k = 512), a theoretical upper bound on the speedup\n",
      "can be obtained under the assumption that FocusPixels can be\n",
      "predicted without any error ( i.e. based on GTs). This speedup\n",
      "bound changes with the minimum chip size and this variation is\n",
      "shown in Fig 12, following the FocusChip generation algorithm\n",
      "2. The same value is used at each scale. For example, reducing\n",
      "the minimum chip size from 512 to 64 can lead to a theoretical\n",
      "speedup of ∼10 times over the baseline which performs inference \n",
      "#@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@#\n",
      "\n",
      "shown in Fig 12, following the FocusChip generation algorithm\n",
      "2. The same value is used at each scale. For example, reducing\n",
      "the minimum chip size from 512 to 64 can lead to a theoretical\n",
      "speedup of ∼10 times over the baseline which performs inference\n",
      "on 3 scales. However, a signiﬁcant reduction in minimum chip\n",
      "size can also affect detection performance - a reasonable amount\n",
      "of context is necessary for retaining high detection accuracy.\n",
      "4.4.2 Quality of FocusPixel prediction\n",
      "Here, we evaluate how well our network predicts FocusPixels at\n",
      "different scales using two criteria. First, we measure the recall for\n",
      "predicting FocusPixels at two different resolutions and show the\n",
      "results in Fig 13 a. This provides us with an upper bound on the\n",
      "accuracy of localizing small objects using low-resolution images.\n",
      "However, not all the ground-truth objects that are annotated might\n",
      "be correctly detected. Since our eventual goal is to accelerate the \n",
      "#@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@#\n",
      "\n",
      "results in Fig 13 a. This provides us with an upper bound on the\n",
      "accuracy of localizing small objects using low-resolution images.\n",
      "However, not all the ground-truth objects that are annotated might\n",
      "be correctly detected. Since our eventual goal is to accelerate the\n",
      "detector, cropping regions that cover ground-truth instances which\n",
      "the detector cannot detect would not be useful. Therefore, the\n",
      "ﬁnal effectiveness of FocusChips is intrinsically coupled with the\n",
      "detector, hence we also report the accuracy of FocusPixel predic-\n",
      "tion on regions which are conﬁdently detected in Fig 13 b. This is\n",
      "achieved by only considering the FocusPixels corresponding to the\n",
      "GT boxes that signiﬁcantly overlap (IoU > 0.5) with a detection-\n",
      "box with a score ≤0.5. At a threshold of 0.5, the detector still\n",
      "obtains an mAP of 47% which is within 1% of the ﬁnal mAP and\n",
      "does not have a high false-positive rate.\n",
      "As expected, we obtain better recall at higher resolutions with \n",
      "#@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@#\n",
      "\n",
      "GT boxes that signiﬁcantly overlap (IoU > 0.5) with a detection-\n",
      "box with a score ≤0.5. At a threshold of 0.5, the detector still\n",
      "obtains an mAP of 47% which is within 1% of the ﬁnal mAP and\n",
      "does not have a high false-positive rate.\n",
      "As expected, we obtain better recall at higher resolutions with\n",
      "both metrics. We can cover all conﬁdent detections at the higher\n",
      "resolution (scale 2) when the predicted FocusPixels cover just 5%\n",
      "of the total image area. At a lower resolution (scale 1), when\n",
      "the FocusPixels cover 25% of the total image area, we cover all\n",
      "conﬁdent detections, see Fig 13 b.\n",
      "4.4.3 Quality of FocusChips\n",
      "Eventually, it’s the FocusChips and not FocusPixels that are\n",
      "input to the network, therefore, we evaluate the accuracy of\n",
      "the generated FocusChips, from the FocusPixels, using similar\n",
      "metrics as in Sec. 4.4.2 - the recall of all GT boxes enclosed by\n",
      "FocusChips and the recall for GT boxes enclosed by FocusChips\n",
      "that overlap with a conﬁdent detection. To achieve perfect recall \n",
      "#@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@#\n",
      "\n",
      "the generated FocusChips, from the FocusPixels, using similar\n",
      "metrics as in Sec. 4.4.2 - the recall of all GT boxes enclosed by\n",
      "FocusChips and the recall for GT boxes enclosed by FocusChips\n",
      "that overlap with a conﬁdent detection. To achieve perfect recall\n",
      "for conﬁdent detections at scale 2, FocusChips cover 5% more\n",
      "area than FocusPixels. At scale 1, they cover 10% more area. This\n",
      "is because objects are often not rectangular in shape. These results\n",
      "are shown in Fig 13 d.\n",
      "4.4.4 Comparison with SNIPER\n",
      "In this section, we compare the efﬁcient multi-scale inference in\n",
      "AutoFocus, with testing on Full Image Pyramids in SNIPER. For\n",
      "fair comparison, in AutoFocus, we just add the FocusPixel predic-\n",
      "tion branch to our detector while keeping everything else the same.\n",
      "Table 9 shows the results. While matching SNIPER’s performance\n",
      "of 47.9% (68.3% at 0.5 IoU), AutoFocus processes 6.4 images per\n",
      "second on the test-dev set with a TitanX Pascal GPU. SNIPER \n",
      "#@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@#\n",
      "\n",
      "tion branch to our detector while keeping everything else the same.\n",
      "Table 9 shows the results. While matching SNIPER’s performance\n",
      "of 47.9% (68.3% at 0.5 IoU), AutoFocus processes 6.4 images per\n",
      "second on the test-dev set with a TitanX Pascal GPU. SNIPER\n",
      "processes 2.5 images per second. Moreover, AutoFocus is able\n",
      "to reduce the average number of pixels processed to half while\n",
      "dropping the AP by just 0.7%. This shows the effectiveness of the\n",
      "chip sampling process in AutoFocus.\n",
      "4.4.5 Speed Accuracy Trade-off for AutoFocus\n",
      "In Section 4.4.4, we showed that AutoFocus inference accuracy\n",
      "matches that of the SNIPER’s. Moreover, as discussed in Section\n",
      "3.4, the speed accuracy trade-off in AutoFocus can be further con-\n",
      "trolled. Therefore, we study the effect of AutoFocus parameters\n",
      "on its inference speed and accuracy. We perform a grid-search\n",
      "\f",
      "15\n",
      "1X 2X 3X 4X 6X\n",
      "Pixels in S1+S2+S3 / AutoFocus Pixels\n",
      "38\n",
      "40\n",
      "42\n",
      "44\n",
      "46\n",
      "48mAP\n",
      " AutoFocus\n",
      "S1+S2+S3\n",
      "S1+S2\n",
      "S2\n",
      "3.6 1.8 1.2 0.9 0.6\n",
      "Milion Pixels Processed\n",
      "(a) \n",
      "#@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@#\n",
      "\n",
      "trolled. Therefore, we study the effect of AutoFocus parameters\n",
      "on its inference speed and accuracy. We perform a grid-search\n",
      "\f",
      "15\n",
      "1X 2X 3X 4X 6X\n",
      "Pixels in S1+S2+S3 / AutoFocus Pixels\n",
      "38\n",
      "40\n",
      "42\n",
      "44\n",
      "46\n",
      "48mAP\n",
      " AutoFocus\n",
      "S1+S2+S3\n",
      "S1+S2\n",
      "S2\n",
      "3.6 1.8 1.2 0.9 0.6\n",
      "Milion Pixels Processed\n",
      "(a)\n",
      "1X 2X 3X 4X 6X\n",
      "Pixels in S1+S2+S3 / AutoFocus Pixels\n",
      "56\n",
      "58\n",
      "60\n",
      "62\n",
      "64\n",
      "66\n",
      "68AP @ 0.5\n",
      " AutoFocus\n",
      "S1+S2+S3\n",
      "S1+S2\n",
      "S2\n",
      "3.6 1.8 1.2 0.9 0.6\n",
      "Milion Pixels Processed (b)\n",
      "1X 5X 10X 15X 20X\n",
      "Pixels in S3 / AutoFocus Pixels in S3\n",
      "45.0\n",
      "45.5\n",
      "46.0\n",
      "46.5\n",
      "47.0\n",
      "47.5\n",
      "48.0mAP\n",
      "AutoFocus\n",
      "S1+S2+S3\n",
      "2.6 0.5 0.3 0.2 0.1\n",
      "Milion Pixels Processed in Scale 3 (c)\n",
      "1X 5X 10X 15X 20X\n",
      "Pixels in S3 / AutoFocus Pixels in S3\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69AP @ 0.5\n",
      "AutoFocus\n",
      "S1+S2+S3\n",
      "2.6 0.5 0.3 0.2 0.1\n",
      "Milion Pixels Processed in Scale 3 (d)\n",
      "Fig. 14: Results are on the val-2017 set. (a,c) show the mAP averaged for IoU from 0.5 to 0.95 with an interval of 0.05 (COCO metric). \n",
      "#@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@#\n",
      "\n",
      "Pixels in S3 / AutoFocus Pixels in S3\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69AP @ 0.5\n",
      "AutoFocus\n",
      "S1+S2+S3\n",
      "2.6 0.5 0.3 0.2 0.1\n",
      "Milion Pixels Processed in Scale 3 (d)\n",
      "Fig. 14: Results are on the val-2017 set. (a,c) show the mAP averaged for IoU from 0.5 to 0.95 with an interval of 0.05 (COCO metric).\n",
      "(b,d) show mAP at 50% overlap (PASCAL metric). We can reduce the number of pixels processed by a factor of 2.8 times without any\n",
      "loss of performance. A 5 times reduction in pixels is obtained with a drop of 1% in mAP.\n",
      "TABLE 9: Comparison between the efﬁcient multi-scale testing\n",
      "in AutoFocus and testing on full image pyramids in SNIPER on\n",
      "COCO test-dev. The average pixels processed over the dataset are\n",
      "also reported.\n",
      "Method Pixels AP AP50 S M L\n",
      "SNIPER 19102 47.9 68.3 31.5 50.5 60.3\n",
      "AutoFocus 11752 47.9 68.3 31.5 50.5 60.3\n",
      "9302 47.2 67.5 30.9 49.0 60.0\n",
      "on the concerned parameters - dilation, min-chip size and the\n",
      "threshold - to generate FocusChips on a subset of 100 images in \n",
      "#@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@#\n",
      "\n",
      "also reported.\n",
      "Method Pixels AP AP50 S M L\n",
      "SNIPER 19102 47.9 68.3 31.5 50.5 60.3\n",
      "AutoFocus 11752 47.9 68.3 31.5 50.5 60.3\n",
      "9302 47.2 67.5 30.9 49.0 60.0\n",
      "on the concerned parameters - dilation, min-chip size and the\n",
      "threshold - to generate FocusChips on a subset of 100 images in\n",
      "the validation set. For a given average number of pixels, we check\n",
      "which conﬁguration of parameters obtains the best mAP on this\n",
      "subset. Since there are two scales at which we predict FocusPixels,\n",
      "we ﬁrst ﬁnd the parameters of AutoFocus when it is only applied\n",
      "to the highest resolution scale. Then we ﬁx these parameters for\n",
      "the highest scale, and ﬁnd parameters for applying AutoFocus at\n",
      "scale 2.\n",
      "In Fig 14 we show that the multi-scale inference baseline\n",
      "which uses 3 scales obtains an mAP of 47.5% (and 68% at 50%\n",
      "overlap) on the val-2017 set. Using only the lower two scales\n",
      "obtains an mAP of 45.4%. The middle scale alone obtains an mAP\n",
      "of 37%. This is partly because the detector is trained with our scale \n",
      "#@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@#\n",
      "\n",
      "which uses 3 scales obtains an mAP of 47.5% (and 68% at 50%\n",
      "overlap) on the val-2017 set. Using only the lower two scales\n",
      "obtains an mAP of 45.4%. The middle scale alone obtains an mAP\n",
      "of 37%. This is partly because the detector is trained with our scale\n",
      "normalization scheme. As a result, the performance on a single\n",
      "scale alone is not very good, although multi-scale performance\n",
      "is high. The maximum savings in pixels which we can obtain\n",
      "while retaining performance is 2.8 times. We lose approximately\n",
      "1% mAP to obtain a 5 times reduction over our baseline in the\n",
      "val-2017 set.\n",
      "We also perform an ablation experiment for the FocusPixels\n",
      "predicted using scale 2. Note that the performance of just using\n",
      "scales 1 and 2 is 45%. We can retain the original performance of\n",
      "47.5% on the val-2017 set by processing just one-ﬁfth of scale\n",
      "3. With a 0.5% drop, we can reduce the pixels processed by 11\n",
      "times in the highest resolution image. This can be improved to 20 \n",
      "#@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@#\n",
      "\n",
      "scales 1 and 2 is 45%. We can retain the original performance of\n",
      "47.5% on the val-2017 set by processing just one-ﬁfth of scale\n",
      "3. With a 0.5% drop, we can reduce the pixels processed by 11\n",
      "times in the highest resolution image. This can be improved to 20\n",
      "times with a 1% drop in mAP, which is still 1.5% better than the\n",
      "performance of the lower two scales.\n",
      "4.5 Comparison with other methods\n",
      "In this section, we compare our methods with other object de-\n",
      "tectors on COCO and Pascal VOC datasets. Table 10 shows the\n",
      "results on COCO test-dev. One has to keep in mind that it is\n",
      "TABLE 10: Comparison on the COCO test-dev. Results for others\n",
      "are taken from the papers/GitHub of the authors. Note that average\n",
      "pixels processed over the dataset are reported (instead of the\n",
      "shorter side). All methods use a ResNet-101 backbone. ‘+’ denotes\n",
      "the multi-scale version provided by the authors.\n",
      "Method Pixels AP AP50 S M L\n",
      "Retina [33] 9502 37.8 57.5 20.2 41.1 49.2\n",
      "D-RFCN [44] 9502 38.4 60.1 18.5 41.6 52.5 \n",
      "#@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@#\n",
      "\n",
      "pixels processed over the dataset are reported (instead of the\n",
      "shorter side). All methods use a ResNet-101 backbone. ‘+’ denotes\n",
      "the multi-scale version provided by the authors.\n",
      "Method Pixels AP AP50 S M L\n",
      "Retina [33] 9502 37.8 57.5 20.2 41.1 49.2\n",
      "D-RFCN [44] 9502 38.4 60.1 18.5 41.6 52.5\n",
      "Mask-RCNN [63] 9502 39.8 62.3 22.1 43.2 51.2\n",
      "FSAF [64] 9502 40.9 61.5 24.0 44.2 51.3\n",
      "LightH [65] 9502 41.5 - 25.2 45.3 53.1\n",
      "FCOS [66] 9502 41.5 60.7 24.4 44.8 51.6\n",
      "Reﬁne+ [67] 31002 41.8 62.9 25.6 45.1 54.1\n",
      "Corner+ [68] 12402 42.1 57.8 20.8 44.8 56.7\n",
      "FoveaBox-align [35] 9502 42.1 62.7 25.2 46.6 54.5\n",
      "Cascade R-CNN [69] 9502 42.8 62.1 23.7 45.5 55.2\n",
      "FSAF+ (+hﬂip) [64] 41002 42.8 63.1 27.8 45.5 53.2\n",
      "RepPoints [70] 9502 45.0 66.1 26.6 48.6 57.5\n",
      "RepPoints+ [70] 28502 46.5 67.4 30.3 49.7 57.1\n",
      "SNIP 19102 44.4 66.2 27.3 47.4 56.9\n",
      "SNIPER 19102 47.9 68.3 31.5 50.5 60.3\n",
      "11752 47.9 68.3 31.5 50.5 60.3\n",
      "AutoFocus 9302 47.2 67.5 30.9 49.0 60.0\n",
      "8602 46.9 67.0 30.1 48.9 60.0 \n",
      "#@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@#\n",
      "\n",
      "RepPoints [70] 9502 45.0 66.1 26.6 48.6 57.5\n",
      "RepPoints+ [70] 28502 46.5 67.4 30.3 49.7 57.1\n",
      "SNIP 19102 44.4 66.2 27.3 47.4 56.9\n",
      "SNIPER 19102 47.9 68.3 31.5 50.5 60.3\n",
      "11752 47.9 68.3 31.5 50.5 60.3\n",
      "AutoFocus 9302 47.2 67.5 30.9 49.0 60.0\n",
      "8602 46.9 67.0 30.1 48.9 60.0\n",
      "TABLE 11: Comparison on the PASCAL VOC 2007 test-set.\n",
      "All methods use ResNet-101 and trained on VOC2012 train-\n",
      "val+VOC2007 trainval. The average pixels processed over the\n",
      "dataset are also reported. To show the robustness of AutoFocus\n",
      "to hyper-parameter choices, in ‘*’ we use the same parameters as\n",
      "COCO and run the algorithm on PASCAL.\n",
      "Method Pixels AP50 AP70\n",
      "Faster RCNN [50] 7052 76.4 -\n",
      "R-FCN [24] 7052 80.5 -\n",
      "C-FRCNN [71] 7052 82.2 -\n",
      "Deformable ConvNet [23] 7052 82.3 67.8\n",
      "CoupleNet [72] 7052 82.7 -\n",
      "FSN [73] 7052 82.9 -\n",
      "Deformable ConvNet v2 [74] 7052 84.9 73.5\n",
      "SNIPER 19152 86.6 80.5\n",
      "AutoFocus* 8602 85.8 79.5\n",
      "AutoFocus 7002 85.3 78.1\n",
      "12502 86.5 80.2\n",
      "\f",
      "16\n",
      "Detections \n",
      "480x512\n",
      "FocusPixels &\n",
      "Chips 480x512\n",
      "Detections \n",
      "800x1280 \n",
      "#@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@#\n",
      "\n",
      "Deformable ConvNet [23] 7052 82.3 67.8\n",
      "CoupleNet [72] 7052 82.7 -\n",
      "FSN [73] 7052 82.9 -\n",
      "Deformable ConvNet v2 [74] 7052 84.9 73.5\n",
      "SNIPER 19152 86.6 80.5\n",
      "AutoFocus* 8602 85.8 79.5\n",
      "AutoFocus 7002 85.3 78.1\n",
      "12502 86.5 80.2\n",
      "\f",
      "16\n",
      "Detections \n",
      "480x512\n",
      "FocusPixels &\n",
      "Chips 480x512\n",
      "Detections \n",
      "800x1280\n",
      "FocusPixels &\n",
      "Chips 800x1280\n",
      "Detection \n",
      "1400x2000\n",
      "Final Dets after \n",
      "Focus Stacking\n",
      "Fig. 15: Each row shows the inference pipeline in AutoFocus. The conﬁdence for FocusPixels and FocusChips are shown in red, and\n",
      "yellow respectively in the second and fourth columns. Detections are shown in green. As can be seen, complex images containing many\n",
      "small objects like the ﬁrst row can generate multiple FocusChips in high resolutions like 1400 ×2000. Images which do not contain\n",
      "small objects are not processed at all in high resolution, like the one in the second row.\n",
      "difﬁcult to fairly compare different detectors as they differ in\n",
      "backbone architectures (like ResNet [25], ResNext [75], Xception \n",
      "#@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@#\n",
      "\n",
      "small objects are not processed at all in high resolution, like the one in the second row.\n",
      "difﬁcult to fairly compare different detectors as they differ in\n",
      "backbone architectures (like ResNet [25], ResNext [75], Xception\n",
      "[76]), pre-training data ( e.g. ImageNet-5k, JFT [77], OpenImages\n",
      "[78]), different structures in the underlying network ( e.g multi-\n",
      "scale features [6], [29], deformable convolutions [23], heavier\n",
      "heads [53], anchor sizes, path aggregation [31]), test time aug-\n",
      "mentations like ﬂipping, mask tightening, iterative bounding box\n",
      "regression etc. In our comparison, we use a ResNet-101 backbone\n",
      "for all methods. Besides AP at different thresholds and different\n",
      "object sizes, we also show the average number of pixels processed\n",
      "by each method during inference.\n",
      "First, among our methods, SNIPER outperforms its baseline\n",
      "SNIP, while improving the training speed noticeably. AutoFocus,\n",
      "on the other hand, exactly matches its baseline SNIPER, in terms \n",
      "#@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@#\n",
      "\n",
      "object sizes, we also show the average number of pixels processed\n",
      "by each method during inference.\n",
      "First, among our methods, SNIPER outperforms its baseline\n",
      "SNIP, while improving the training speed noticeably. AutoFocus,\n",
      "on the other hand, exactly matches its baseline SNIPER, in terms\n",
      "of AP, however, reduces the number of pixels processed by more\n",
      "than 2X. In terms of absolute clock time, SNIPER processes 2.5\n",
      "images per second. AutoFocus increases the speed to 6.4 images\n",
      "per second. To compare, RetinaNet with a ResNet-101 backbone\n",
      "and a FPN architecture processes 6.3 images per second on a P100\n",
      "GPU (which is like Titan X), but obtains 37.8% mAP 3. As can be\n",
      "seen, AutoFocus can effectively reduce the number of processed\n",
      "pixels further to 8602 while still achieving higher APs compared\n",
      "to multi-scale methods such as RepPoins+ [70] which on average\n",
      "processes around 11×more pixels.\n",
      "We also report results on the PASCAL VOC dataset in \n",
      "#@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@#\n",
      "\n",
      "seen, AutoFocus can effectively reduce the number of processed\n",
      "pixels further to 8602 while still achieving higher APs compared\n",
      "to multi-scale methods such as RepPoins+ [70] which on average\n",
      "processes around 11×more pixels.\n",
      "We also report results on the PASCAL VOC dataset in\n",
      "Table 11. To show the robustness of AutoFocus to its hyper-\n",
      "parameters, we use exactly the same hyper-parameters tuned for\n",
      "COCO (shown as AutoFocus*). While processing the same area\n",
      "as DeformableV2 [74], AutoFocus achieves 4.6% better AP at\n",
      "0.7 IoU. It also matches the performance of SNIPER while being\n",
      "considerably more efﬁcient.\n",
      "5 C ONCLUSION\n",
      "We provided critical insights into the popular single-scale object-\n",
      "detection paradigm and highlighted some of its detrimental limi-\n",
      "tations. Carefully designed experiments showed that large scale-\n",
      "variation in object sizes adversely affects both the training and\n",
      "3. https://github.com/facebookresearch/Detectron/blob/master/MODEL\n",
      "ZOO.md \n",
      "#@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@#\n",
      "\n",
      "detection paradigm and highlighted some of its detrimental limi-\n",
      "tations. Carefully designed experiments showed that large scale-\n",
      "variation in object sizes adversely affects both the training and\n",
      "3. https://github.com/facebookresearch/Detectron/blob/master/MODEL\n",
      "ZOO.md\n",
      "inference performance for object detection. Based on the charac-\n",
      "teristics of the human foveal-vision system and scale-space theory,\n",
      "scale-normalized image pyramids are proposed as an effective\n",
      "tool to tackle the aforementioned scale-variation and its effec-\n",
      "tiveness is showed on multiple popular object-detection systems.\n",
      "Generalizable guidelines are also provided to implement scale-\n",
      "normalization based on the input image, network architecture and\n",
      "objects of interest that can be further used for other applications\n",
      "as well. Our proposed technique to perform efﬁcient spatial and\n",
      "scale-space sub-sampling of salient regions resulted in 3 ×faster\n",
      "training and 10 ×reduction in memory complexity which coun- \n",
      "#@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@#\n",
      "\n",
      "objects of interest that can be further used for other applications\n",
      "as well. Our proposed technique to perform efﬁcient spatial and\n",
      "scale-space sub-sampling of salient regions resulted in 3 ×faster\n",
      "training and 10 ×reduction in memory complexity which coun-\n",
      "tered the increased computational complexity introduced by the\n",
      "scale-normalized image pyramid. The reduced memory complex-\n",
      "ity also enabled the use of batch-normalization which improved\n",
      "the results further, leading to state-of-the-art performance on\n",
      "the COCO benchmark. Finally, we presented an active foveal\n",
      "vision-system that processes the image pyramid in a coarse-to-\n",
      "ﬁne manner to predict the location of object-like regions in the\n",
      "ﬁner resolution scales, which speeds up inference by 3 ×resulting\n",
      "in near real-time detection on commodity GPUs.\n",
      "ACKNOWLEDGMENTS\n",
      "The authors would like to thank an Amazon Machine Learning gift\n",
      "for the AWS credits used for this research. The research is based \n",
      "#@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@#\n",
      "\n",
      "ﬁner resolution scales, which speeds up inference by 3 ×resulting\n",
      "in near real-time detection on commodity GPUs.\n",
      "ACKNOWLEDGMENTS\n",
      "The authors would like to thank an Amazon Machine Learning gift\n",
      "for the AWS credits used for this research. The research is based\n",
      "upon work supported by the Ofﬁce of the Director of National\n",
      "Intelligence (ODNI), Intelligence Advanced Research Projects\n",
      "Activity (IARPA), via DOI/IBC Contract Numbers D17PC00287\n",
      "and D17PC00345. The U.S. Government is authorized to re-\n",
      "produce and distribute reprints for Governmental purposes not\n",
      "withstanding any copyright annotation thereon. Disclaimer: The\n",
      "views and conclusions contained herein are those of the authors\n",
      "and should not be interpreted as necessarily representing the\n",
      "ofﬁcial policies or endorsements, either expressed or implied of\n",
      "IARPA, DOI/IBC or the U.S. Government. \n",
      "#@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@#\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " ...]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sep=\"\\n#@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@##@#@#@#@#@#@#\\n\"\n",
    "[print(data.chunk[i],sep) for i in range(len(data))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71814000-98af-42b6-9b20-daa2b8511b6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
